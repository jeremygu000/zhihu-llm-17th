```py
def get_top_n_words(corpus, n=1, k=None):
    """
    Extracts and returns the top k most frequent n-grams (e.g., words, bigrams, trigrams)
    from a given text corpus.

    Parameters
    ----------
    corpus : list[str]
        A list of text documents (each element is a string). Example:
        ["I love cats", "I love dogs too"]

    n : int, optional (default=1)
        The 'n' in n-gram.
        - n=1 â†’ unigrams (single words)
        - n=2 â†’ bigrams (two-word phrases)
        - n=3 â†’ trigrams (three-word phrases), etc.

    k : int or None, optional (default=None)
        The number of top frequent words or n-grams to return.
        If None, returns all words sorted by frequency.

    Returns
    -------
    list[tuple[str, int]]
        A list of tuples: [(word_or_ngram, frequency), ...]
        Sorted by frequency in descending order.

    Example
    -------
    >>> corpus = ["I love cats", "I love dogs too"]
    >>> get_top_n_words(corpus, n=1, k=3)
    [('love', 2), ('i', 2), ('cats', 1)]
    """

    # Initialize a CountVectorizer to count occurrences of n-grams in the corpus.
    # - ngram_range=(n, n): ensures we only consider n-grams of length exactly 'n'.
    # - stop_words: removes common English stopwords (like 'the', 'and', 'is').
    vec = CountVectorizer(ngram_range=(n, n), stop_words=list(ENGLISH_STOPWORDS)).fit(corpus)

    # Transform the corpus into a bag-of-words matrix (documents Ã— n-grams)
    # Each entry represents how many times an n-gram appears in that document.
    bag_of_words = vec.transform(corpus)

    # Sum all columns (axis=0) to get total count of each n-gram across all documents.
    sum_words = bag_of_words.sum(axis=0)

    # Build a list of (word_or_ngram, frequency) pairs.
    # vec.vocabulary_ maps each n-gram to its column index in the bag_of_words matrix.
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]

    # Sort by frequency in descending order (most frequent first).
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

    # Return only the top 'k' results (or all if k=None).
    return words_freq[:k]
```

## What is CountVectorizer ##

ğŸ’¡ ç®€å•è§£é‡Š

CountVectorizer æ˜¯ Scikit-learn é‡Œä¸€ä¸ªæŠŠ**æ–‡æœ¬ï¼ˆtextï¼‰è½¬æ¢æˆæ•°å­—ç‰¹å¾(numeric features)**çš„å·¥å…·ã€‚
å®ƒçš„ä¸»è¦ä½œç”¨æ˜¯æŠŠå¥å­æˆ–æ–‡ç« å˜æˆâ€œè¯é¢‘å‘é‡â€ï¼ˆbag-of-words representationï¼‰ã€‚

é€šä¿—æ¥è¯´ï¼Œå®ƒåšäº†ä¸‰ä»¶äº‹ï¼š

### 1. åˆ†è¯ (Tokenization)ï¼š###
æŠŠæ¯ç¯‡æ–‡æœ¬æ‹†åˆ†æˆå•è¯æˆ–çŸ­è¯­ï¼ˆæ ¹æ®ä½ è®¾å®šçš„ ngram_rangeï¼‰ã€‚
ä¾‹å¦‚ï¼š

```
["I love cats", "I love dogs too"]
```

ğŸŸ© unigramï¼ˆå•è¯çº§åˆ«ï¼‰

ngram_range=(1, 1)
æ¯æ¬¡å–ä¸€ä¸ªè¯ã€‚

```
["i", "love", "cats", "dogs", "too"]
```

ğŸŸ¦ bigramï¼ˆåŒè¯ç»„åˆï¼‰

ngram_range=(2, 2)
æ¯æ¬¡å–ä¸¤ä¸ªè¿ç»­çš„è¯ã€‚

```
["i love", "love cats", "love dogs", "dogs too"]
```

ğŸŸ§ trigramï¼ˆä¸‰è¯ç»„åˆï¼‰

ngram_range=(3, 3)
æ¯æ¬¡å–ä¸‰ä¸ªè¿ç»­çš„è¯ã€‚

```
["i love cats", "i love dogs", "love dogs too"]
```

#### 2. å»ºç«‹è¯è¡¨ (Build vocabulary) ####

ç»Ÿè®¡æ‰€æœ‰å‡ºç°è¿‡çš„è¯æˆ–çŸ­è¯­ï¼Œå¹¶ç»™æ¯ä¸ªåˆ†é…ä¸€ä¸ªå”¯ä¸€ç¼–å·ã€‚

{'i': 0, 'love': 1, 'cats': 2, 'dogs': 3, 'too': 4}


#### 3. ç»Ÿè®¡è¯é¢‘ (Count occurrences)ï¼š####
æ•°å‡ºæ¯ä¸ªè¯åœ¨æ¯ç¯‡æ–‡æœ¬ä¸­å‡ºç°äº†å‡ æ¬¡ï¼Œç”Ÿæˆä¸€ä¸ªâ€œç¨€ç–çŸ©é˜µâ€ï¼ˆsparse matrixï¼‰ã€‚
æ¯”å¦‚ï¼š

- æ–‡æœ¬1: I love cats
- æ–‡æœ¬2: I love dogs dogs too


è½¬æˆçŸ©é˜µï¼š

|     | i | love | cats | dogs | too |
| --- | - | ---- | ---- | ---- | --- |
| æ–‡æœ¬1 | 1 | 1    | 1    | 0    | 0   |
| æ–‡æœ¬2 | 1 | 1    | 0    | 2    | 1   |


ğŸ§© å‚æ•°é‡ç‚¹
| å‚æ•°                            | è¯´æ˜                                               |
| ----------------------------- | ------------------------------------------------ |
| `ngram_range=(1, 1)`          | è¡¨ç¤ºåªç»Ÿè®¡å•ä¸ªè¯ï¼ˆunigramï¼‰ï¼›å¦‚æœè®¾ `(2, 2)` å°±æ˜¯ç»Ÿè®¡åŒè¯ç»„åˆï¼ˆbigramï¼‰ã€‚ |
| `stop_words='english'` æˆ–è‡ªå®šä¹‰åˆ—è¡¨ | æŒ‡å®šå¸¸è§çš„åœç”¨è¯ï¼ˆå¦‚ *the, is, and*ï¼‰åœ¨ç»Ÿè®¡æ—¶è¢«å¿½ç•¥ã€‚               |
| `max_features`                | é™åˆ¶æœ€å¤šä¿ç•™å¤šå°‘ä¸ªé«˜é¢‘è¯ã€‚                                    |
| `min_df` / `max_df`           | æ§åˆ¶æŸä¸ªè¯åœ¨å¤šå°‘æ–‡æ¡£ä¸­å‡ºç°æ‰è¢«ç»Ÿè®¡ï¼Œè¿‡æ»¤æ‰å¤ªå°‘æˆ–å¤ªå¸¸è§çš„è¯ã€‚                   |

ğŸ§  è¾“å‡ºç»“æœ
`CountVectorizer.fit_transform(corpus)`

è¿”å›ä¸€ä¸ªç¨€ç–çŸ©é˜µï¼ˆæ¯è¡Œä»£è¡¨ä¸€ç¯‡æ–‡æ¡£ï¼ˆåœ¨è¿™ä¸ªæ¡ˆä¾‹æ˜¯CSVæ–‡ä»¶ä¸­çš„ä¸€è¡Œï¼‰ï¼Œæ¯åˆ—ä»£è¡¨ä¸€ä¸ªè¯ï¼‰ï¼Œæ•°å€¼æ˜¯è¯¥è¯åœ¨è¯¥æ–‡æ¡£å‡ºç°çš„æ¬¡æ•°ã€‚

ğŸ§¾ å°ä¾‹å­

```py
from sklearn.feature_extraction.text import CountVectorizer

corpus = ["I love cats", "I love dogs too"]
vec = CountVectorizer()
X = vec.fit_transform(corpus)

print(vec.get_feature_names_out())  # æŸ¥çœ‹è¯è¡¨
# ['cats' 'dogs' 'i' 'love' 'too']

print(X.toarray())  # æŸ¥çœ‹è¯é¢‘çŸ©é˜µ
# [[1 0 1 1 0]
#  [0 1 1 1 1]]
```

#### CountVectorizeræˆ–TfidfVectorizerçš„å®ä¾‹ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹é‡è¦å†…å®¹ï¼š####

# 1. vocabulary_ - è¯æ±‡è¡¨å­—å…¸
#    æ ¼å¼ï¼š{è¯æ±‡: ç´¢å¼•ä½ç½®}
# vec.vocabulary_.items() è¿”å›çš„æ˜¯ï¼š{è¯æ±‡: è¯é¢‘ç´¢å¼•} çš„å­—å…¸æ˜ å°„ï¼Œ ä¾‹å¦‚ dict_items([('word1', 0), ('word2', 1), ...])
vocabulary = vec.vocabulary_  

# 2. get_feature_names_out() - ç‰¹å¾åç§°åˆ—è¡¨
#    è¿”å›æŒ‰ç´¢å¼•æ’åºçš„è¯æ±‡åˆ—è¡¨ï¼Œ ä¾‹å¦‚ ['word1', 'word2', ...]
feature_names = vec.get_feature_names_out()

# 3. transform() æ–¹æ³• - æ–‡æœ¬å‘é‡åŒ–
#    å°†æ–‡æœ¬è½¬æ¢ä¸ºç¨€ç–çŸ©é˜µè¡¨ç¤º
transformed_matrix = vec.transform(texts)

# 4. fit_transform() æ–¹æ³• - è®­ç»ƒå¹¶è½¬æ¢
#    å…ˆæ‹Ÿåˆè¯æ±‡è¡¨ï¼Œå†è¿›è¡Œå‘é‡åŒ–
fitted_transformed = vec.fit_transform(texts)

# 5. fit() æ–¹æ³• - æ‹Ÿåˆè¯æ±‡è¡¨
#    æ ¹æ®è®­ç»ƒæ•°æ®å»ºç«‹è¯æ±‡è¡¨
vec.fit(texts)

# 6. å‘é‡åŒ–ç»“æœçš„å½¢çŠ¶ä¿¡æ¯
#    çŸ©é˜µç»´åº¦ï¼š(æ–‡æ¡£æ•° Ã— è¯æ±‡è¡¨å¤§å°)
matrix_shape = vec.transform(texts).shape

# 7. å…¶ä»–é‡è¦å±æ€§ï¼š
#    - vocabulary_ï¼šè¯æ±‡åˆ°ç´¢å¼•çš„æ˜ å°„
#    - stop_words_ï¼šåœç”¨è¯é›†åˆï¼ˆå¦‚æœä½¿ç”¨äº†åœç”¨è¯ï¼‰
#    - n_features_ï¼šç‰¹å¾æ•°é‡


### ç»Ÿè®¡è¯é¢‘ (Count occurrences)ï¼š###

```py
    # Sum all columns (axis=0) to get total count of each n-gram across all documents.
    sum_words = bag_of_words.sum(axis=0)
```
bag_of_words: è¿™æ˜¯ä¸€ä¸ªè¯è¢‹æ¨¡å‹çš„çŸ©é˜µï¼Œé€šå¸¸å½¢çŠ¶ä¸º (æ–‡æ¡£æ•°, è¯æ±‡æ•°)ã€‚åœ¨è¿™ä¸ªæ¡ˆä¾‹å½“ä¸­ï¼Œæ–‡æ¡£æ•°ä¸ºcsvçš„è¡Œæ•°ï¼Œ152è¡Œï¼Œå³CSVå½“ä¸­152ä¸ªé…’åº—çš„desc. è€Œ3-gramçš„è¯æ±‡æ•°ä¸º13908. æ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„n-gramï¼ˆè¯ç»„ï¼‰ï¼Œä¹Ÿæ˜¯è¿™ä¸ªæ¨¡å‹çš„ä¸€ä¸ªç‰¹å¾å€¼ã€‚

   axis=0: è¿™ä¸ªå‚æ•°æŒ‡å®šæ²¿ç€è¡Œçš„æ–¹å‘è¿›è¡Œæ±‚å’Œï¼Œå¯¹æ¯ä¸€åˆ—ï¼ˆæ¯ä¸ªn-gramï¼‰çš„æ‰€æœ‰è¡Œè¿›è¡ŒåŠ æ€»ï¼Œ æœ€åå¾—åˆ°ä¸€ä¸ªé•¿åº¦ä¸º13908çš„æ•°ç»„ï¼Œè¡¨ç¤ºæ¯ä¸ªn-gramåœ¨æ•´ä¸ªè¯æ±‡è¡¨ä¸­å‡ºç°çš„æ€»æ¬¡æ•°ã€‚ ä¹Ÿå°±æ˜¯æ¯ä¸ªç‰¹å¾å€¼çš„æ€»å‡ºç°æ¬¡æ•°ã€‚ ä¹Ÿå°±æ˜¯æ¯ä¸ªç‰¹å¾å€¼çš„æ€»é¢‘ã€‚

å‡è®¾ bag_of_words çŸ©é˜µæ˜¯è¿™æ ·çš„ï¼ˆ3ä¸ªæ–‡æ¡£ï¼Œ4ä¸ªn-gramï¼‰ï¼š

```py
        n-gram_A  n-gram_B  n-gram_C  n-gram_D
æ–‡æ¡£1:     [2]        [0]        [1]        [3]
æ–‡æ¡£2:     [1]        [2]        [0]        [1]  
æ–‡æ¡£3:     [0]        [1]        [2]        [2]

```

æ‰§è¡Œ bag_of_words.sum(axis=0) åï¼š

n-gram_A: 2 + 1 + 0 = 3
n-gram_B: 0 + 2 + 1 = 3
n-gram_C: 1 + 0 + 2 = 3
n-gram_D: 3 + 1 + 2 = 6

ç»“æœï¼šsum_words = [3, 3, 3, 6]

æ³¨æ„ï¼šæ­¤æ—¶sum_wordsè¿˜æ˜¯ä¸€ä¸ªç¨€ç–çŸ©é˜µï¼Œä½†å®ƒçš„ç»´åº¦å‘ç”Ÿäº†å˜åŒ–ï¼š
# - bag_of_words: (æ–‡æ¡£æ•° Ã— è¯æ±‡è¡¨å¤§å°) çš„ç¨€ç–çŸ©é˜µ
# - sum_words: (1 Ã— è¯æ±‡è¡¨å¤§å°) çš„ç¨€ç–çŸ©é˜µ

#### å®é™…æ„ä¹‰ ####
è¿™ä¸€æ­¥çš„ç›®çš„æ˜¯ï¼š

- ç»Ÿè®¡å…¨å±€é¢‘ç‡ï¼šè®¡ç®—æ¯ä¸ªn-gramåœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„æ€»å‡ºç°æ¬¡æ•°
- ç‰¹å¾å·¥ç¨‹ï¼šä¸ºåç»­çš„æ–‡æœ¬åˆ†æã€æœºå™¨å­¦ä¹ æ¨¡å‹æä¾›åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
- é™ç»´å¤„ç†ï¼šä»æ–‡æ¡£çº§åˆ«çš„è¯é¢‘çŸ©é˜µè½¬æ¢ä¸ºè¯æ±‡çº§åˆ«çš„é¢‘ç‡ç»Ÿè®¡

#### çŸ©é˜µè§£é‡Š ####

æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ–‡æ¡£ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„n-gramï¼š

æ–‡æ¡£1ï¼šn-gram_Aå‡ºç°2æ¬¡ï¼Œn-gram_Bå‡ºç°0æ¬¡ï¼Œn-gram_Cå‡ºç°1æ¬¡ï¼Œn-gram_Då‡ºç°3æ¬¡
æ–‡æ¡£2ï¼šn-gram_Aå‡ºç°1æ¬¡ï¼Œn-gram_Bå‡ºç°2æ¬¡ï¼Œn-gram_Cå‡ºç°0æ¬¡ï¼Œn-gram_Då‡ºç°1æ¬¡
æ–‡æ¡£3ï¼šn-gram_Aå‡ºç°0æ¬¡ï¼Œn-gram_Bå‡ºç°1æ¬¡ï¼Œn-gram_Cå‡ºç°2æ¬¡ï¼Œn-gram_Då‡ºç°2æ¬¡

è¿™æ ·å°±å¾—åˆ°äº†æ¯ä¸ªæ–‡æ¡£ä¸­å„ä¸ªn-gramçš„ç²¾ç¡®è¯é¢‘ç»Ÿè®¡ã€‚
- æ¯ä¸ªæ–‡æ¡£çš„æ¯ä¸ªn-graméƒ½æœ‰å…·ä½“çš„å‡ºç°æ¬¡æ•°
- è¿™äº›è®¡æ•°åæ˜ äº†å®é™…æ–‡æœ¬ä¸­è¯æ±‡çš„é¢‘ç‡åˆ†å¸ƒ
- é€šè¿‡ sum(axis=0) æ“ä½œï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ¯ä¸ªn-gramåœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„æ€»å‡ºç°æ¬¡æ•°



```py
 # Build a list of (word_or_ngram, frequency) pairs.
    # vec.vocabulary_ is a dictionary that maps each unique n-gram (word or phrase) 
    # to its corresponding column index in the bag_of_words matrix. 
    # This mapping allows us to efficiently access the frequency count of each n-gram 
    # by using the column index retrieved from this dictionary.
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
```
è¿™è¡Œä»£ç çš„æ„æ€æ˜¯ï¼š

éå† vec.vocabulary_ ä¸­çš„æ¯ä¸€ä¸ª (feature_name, feature_idx) å¯¹ã€‚
é€šè¿‡ç´¢å¼• feature_idxï¼Œä» sum_words[0, idx] è·å–è¯¥featureï¼ˆn-gramï¼‰çš„æ€»å‡ºç°æ¬¡æ•°ã€‚
sum_words[0, idx] è¡¨ç¤ºå–ç¨€ç–çŸ©é˜µ sum_words çš„ç¬¬ 0 è¡Œã€ç¬¬ idx åˆ—çš„å€¼ã€‚
ç„¶åå°† (feature_name, count) ç»„æˆå…ƒç»„ï¼Œå­˜å…¥ words_freq åˆ—è¡¨ã€‚

ç›¸å½“äº

```py
# Build a list of (word_or_ngram, frequency) pairs using a simple loop
words_freq = []
for word, idx in vec.vocabulary_.items():
    # Extract frequency for this specific feature using the index
    freq = sum_words[0, idx]
    words_freq.append((word, freq))
```