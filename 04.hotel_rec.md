```py
def get_top_n_words(corpus, n=1, k=None):
    """
    Extracts and returns the top k most frequent n-grams (e.g., words, bigrams, trigrams)
    from a given text corpus.

    Parameters
    ----------
    corpus : list[str]
        A list of text documents (each element is a string). Example:
        ["I love cats", "I love dogs too"]

    n : int, optional (default=1)
        The 'n' in n-gram.
        - n=1 → unigrams (single words)
        - n=2 → bigrams (two-word phrases)
        - n=3 → trigrams (three-word phrases), etc.

    k : int or None, optional (default=None)
        The number of top frequent words or n-grams to return.
        If None, returns all words sorted by frequency.

    Returns
    -------
    list[tuple[str, int]]
        A list of tuples: [(word_or_ngram, frequency), ...]
        Sorted by frequency in descending order.

    Example
    -------
    >>> corpus = ["I love cats", "I love dogs too"]
    >>> get_top_n_words(corpus, n=1, k=3)
    [('love', 2), ('i', 2), ('cats', 1)]
    """

    # Initialize a CountVectorizer to count occurrences of n-grams in the corpus.
    # - ngram_range=(n, n): ensures we only consider n-grams of length exactly 'n'.
    # - stop_words: removes common English stopwords (like 'the', 'and', 'is').
    vec = CountVectorizer(ngram_range=(n, n), stop_words=list(ENGLISH_STOPWORDS)).fit(corpus)

    # Transform the corpus into a bag-of-words matrix (documents × n-grams)
    # Each entry represents how many times an n-gram appears in that document.
    bag_of_words = vec.transform(corpus)

    # Sum all columns (axis=0) to get total count of each n-gram across all documents.
    sum_words = bag_of_words.sum(axis=0)

    # Build a list of (word_or_ngram, frequency) pairs.
    # vec.vocabulary_ maps each n-gram to its column index in the bag_of_words matrix.
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]

    # Sort by frequency in descending order (most frequent first).
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

    # Return only the top 'k' results (or all if k=None).
    return words_freq[:k]
```

## What is CountVectorizer ##

💡 简单解释

CountVectorizer 是 Scikit-learn 里一个把**文本（text）转换成数字特征(numeric features)**的工具。
它的主要作用是把句子或文章变成“词频向量”（bag-of-words representation）。

通俗来说，它做了三件事：

### 1. 分词 (Tokenization)：###
把每篇文本拆分成单词或短语（根据你设定的 ngram_range）。
例如：

```
["I love cats", "I love dogs too"]
```

🟩 unigram（单词级别）

ngram_range=(1, 1)
每次取一个词。

```
["i", "love", "cats", "dogs", "too"]
```

🟦 bigram（双词组合）

ngram_range=(2, 2)
每次取两个连续的词。

```
["i love", "love cats", "love dogs", "dogs too"]
```

🟧 trigram（三词组合）

ngram_range=(3, 3)
每次取三个连续的词。

```
["i love cats", "i love dogs", "love dogs too"]
```

#### 2. 建立词表 (Build vocabulary) ####

统计所有出现过的词或短语，并给每个分配一个唯一编号。

{'i': 0, 'love': 1, 'cats': 2, 'dogs': 3, 'too': 4}


#### 3. 统计词频 (Count occurrences)：####
数出每个词在每篇文本中出现了几次，生成一个“稀疏矩阵”（sparse matrix）。
比如：

- 文本1: I love cats
- 文本2: I love dogs dogs too


转成矩阵：

|     | i | love | cats | dogs | too |
| --- | - | ---- | ---- | ---- | --- |
| 文本1 | 1 | 1    | 1    | 0    | 0   |
| 文本2 | 1 | 1    | 0    | 2    | 1   |


🧩 参数重点
| 参数                            | 说明                                               |
| ----------------------------- | ------------------------------------------------ |
| `ngram_range=(1, 1)`          | 表示只统计单个词（unigram）；如果设 `(2, 2)` 就是统计双词组合（bigram）。 |
| `stop_words='english'` 或自定义列表 | 指定常见的停用词（如 *the, is, and*）在统计时被忽略。               |
| `max_features`                | 限制最多保留多少个高频词。                                    |
| `min_df` / `max_df`           | 控制某个词在多少文档中出现才被统计，过滤掉太少或太常见的词。                   |

🧠 输出结果
`CountVectorizer.fit_transform(corpus)`

返回一个稀疏矩阵（每行代表一篇文档（在这个案例是CSV文件中的一行），每列代表一个词），数值是该词在该文档出现的次数。

🧾 小例子

```py
from sklearn.feature_extraction.text import CountVectorizer

corpus = ["I love cats", "I love dogs too"]
vec = CountVectorizer()
X = vec.fit_transform(corpus)

print(vec.get_feature_names_out())  # 查看词表
# ['cats' 'dogs' 'i' 'love' 'too']

print(X.toarray())  # 查看词频矩阵
# [[1 0 1 1 0]
#  [0 1 1 1 1]]
```

#### CountVectorizer或TfidfVectorizer的实例主要包括以下重要内容：####

# 1. vocabulary_ - 词汇表字典
#    格式：{词汇: 索引位置}
# vec.vocabulary_.items() 返回的是：{词汇: 词频索引} 的字典映射， 例如 dict_items([('word1', 0), ('word2', 1), ...])
vocabulary = vec.vocabulary_  

# 2. get_feature_names_out() - 特征名称列表
#    返回按索引排序的词汇列表， 例如 ['word1', 'word2', ...]
feature_names = vec.get_feature_names_out()

# 3. transform() 方法 - 文本向量化
#    将文本转换为稀疏矩阵表示
transformed_matrix = vec.transform(texts)

# 4. fit_transform() 方法 - 训练并转换
#    先拟合词汇表，再进行向量化
fitted_transformed = vec.fit_transform(texts)

# 5. fit() 方法 - 拟合词汇表
#    根据训练数据建立词汇表
vec.fit(texts)

# 6. 向量化结果的形状信息
#    矩阵维度：(文档数 × 词汇表大小)
matrix_shape = vec.transform(texts).shape

# 7. 其他重要属性：
#    - vocabulary_：词汇到索引的映射
#    - stop_words_：停用词集合（如果使用了停用词）
#    - n_features_：特征数量


### 统计词频 (Count occurrences)：###

```py
    # Sum all columns (axis=0) to get total count of each n-gram across all documents.
    sum_words = bag_of_words.sum(axis=0)
```
bag_of_words: 这是一个词袋模型的矩阵，通常形状为 (文档数, 词汇数)。在这个案例当中，文档数为csv的行数，152行，即CSV当中152个酒店的desc. 而3-gram的词汇数为13908. 每一列代表一个特定的n-gram（词组），也是这个模型的一个特征值。

   axis=0: 这个参数指定沿着行的方向进行求和，对每一列（每个n-gram）的所有行进行加总， 最后得到一个长度为13908的数组，表示每个n-gram在整个词汇表中出现的总次数。 也就是每个特征值的总出现次数。 也就是每个特征值的总频。

假设 bag_of_words 矩阵是这样的（3个文档，4个n-gram）：

```py
        n-gram_A  n-gram_B  n-gram_C  n-gram_D
文档1:     [2]        [0]        [1]        [3]
文档2:     [1]        [2]        [0]        [1]  
文档3:     [0]        [1]        [2]        [2]

```

执行 bag_of_words.sum(axis=0) 后：

n-gram_A: 2 + 1 + 0 = 3
n-gram_B: 0 + 2 + 1 = 3
n-gram_C: 1 + 0 + 2 = 3
n-gram_D: 3 + 1 + 2 = 6

结果：sum_words = [3, 3, 3, 6]

注意：此时sum_words还是一个稀疏矩阵，但它的维度发生了变化：
# - bag_of_words: (文档数 × 词汇表大小) 的稀疏矩阵
# - sum_words: (1 × 词汇表大小) 的稀疏矩阵

#### 实际意义 ####
这一步的目的是：

- 统计全局频率：计算每个n-gram在整个语料库中的总出现次数
- 特征工程：为后续的文本分析、机器学习模型提供基础统计信息
- 降维处理：从文档级别的词频矩阵转换为词汇级别的频率统计

#### 矩阵解释 ####

每一行代表一个文档，每一列代表一个特定的n-gram：

文档1：n-gram_A出现2次，n-gram_B出现0次，n-gram_C出现1次，n-gram_D出现3次
文档2：n-gram_A出现1次，n-gram_B出现2次，n-gram_C出现0次，n-gram_D出现1次
文档3：n-gram_A出现0次，n-gram_B出现1次，n-gram_C出现2次，n-gram_D出现2次

这样就得到了每个文档中各个n-gram的精确词频统计。
- 每个文档的每个n-gram都有具体的出现次数
- 这些计数反映了实际文本中词汇的频率分布
- 通过 sum(axis=0) 操作，我们得到了每个n-gram在整个语料库中的总出现次数



```py
 # Build a list of (word_or_ngram, frequency) pairs.
    # vec.vocabulary_ is a dictionary that maps each unique n-gram (word or phrase) 
    # to its corresponding column index in the bag_of_words matrix. 
    # This mapping allows us to efficiently access the frequency count of each n-gram 
    # by using the column index retrieved from this dictionary.
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
```
这行代码的意思是：

遍历 vec.vocabulary_ 中的每一个 (feature_name, feature_idx) 对。
通过索引 feature_idx，从 sum_words[0, idx] 获取该feature（n-gram）的总出现次数。
sum_words[0, idx] 表示取稀疏矩阵 sum_words 的第 0 行、第 idx 列的值。
然后将 (feature_name, count) 组成元组，存入 words_freq 列表。

相当于

```py
# Build a list of (word_or_ngram, frequency) pairs using a simple loop
words_freq = []
for word, idx in vec.vocabulary_.items():
    # Extract frequency for this specific feature using the index
    freq = sum_words[0, idx]
    words_freq.append((word, freq))
```

```py
df.set_index('name', inplace = True)

# Extract text features using TF-IDF with custom stop words list, 1-gram features (1000) + 2-gram features (1200) + 3-gram features (1147) = 3347 total
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0.01, stop_words=list(ENGLISH_STOP_WORDS))

# Extract TF-IDF for desc_clean column
tfidf_matrix = tf.fit_transform(df['desc_clean'])
```

TF-IDF 的全称是 Term Frequency-Inverse Document Frequency，即词频-逆文档频率。
它是一种用于信息检索与文本挖掘的统计方法，用以评估一个词语对于一个文件集或一个语料库中的其中一份文件的重要程度。
核心思想：一个词语在一篇文章中出现的次数（TF）越多，同时在所有文档中出现的次数（DF）越少，越能代表该文章。这意味着，TF-IDF倾向于过滤掉常见的词语，保留重要的、有区分度的词语。

实际例子
假设我们有以下酒店描述：

```Text
酒店A：豪华海景房，免费WiFi，靠近海滩
酒店B：经济型房间，基础设施，市中心位置  
酒店C：高端海景房，免费WiFi，私人阳台
```

TF-IDF会这样处理：

```text
"豪华"、"经济型" → IDF低（常见词，不重要）
"海景房"、"私人阳台" → IDF高（稀有词，重要）
"免费WiFi" → IDF中等（常见但有用）
```

#### 三、 为什么稀有词更重要？
- 区分性强：能帮助识别特定特征
- 描述性好：很少出现的词往往代表独特属性
- 信息量大：稀有词汇承载更多信息

词频高 = 重要性低

大家都有的特性（如"空调"、"热水"）
缺乏区分度，不能帮助识别独特特征
因此权重较低
词频低 = 重要性高

独特的特色属性（如"海景房"、"私人阳台"）
具有区分度，能帮助识别特定酒店
因此权重较高
在酒店推荐中的应用

```text
酒店A：经济型，基础设施，市中心
酒店B：豪华海景房，免费WiFi，私人阳台
酒店C：商务型，会议设施，机场附近
```

通过TF-IDF向量化后：
```text
低权重词：空调、热水（常见，不重要）
高权重词：海景房、私人阳台（独特，重要）
```

这样系统就能：

1. 识别出酒店的独特卖点
2. 根据用户偏好匹配相似特征的酒店
3. 提供更精准的个性化推荐

因此，TF-IDF将两者结合，完美地解决了这个问题：

TF 部分负责找出当前文档的关键词（在当前文档中出现频繁）。
IDF 部分负责降低常见词的权重，提升特征词的权重。


```py
# 计算酒店之间的余弦相似度（线性核函数）
# 线性核函数 linear_kernel() 计算的是两个矩阵之间的内积
# 当两个参数相同时，计算的是每个向量与自身以及其他所有向量的内积
# 这正好对应余弦相似度的计算
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)

# 由于tfidf_matrix的形状为(152, 3173)，其中152表示有152个酒店样本
# 当使用linear_kernel(tfidf_matrix, tfidf_matrix)时，函数会计算第一个矩阵的每一行与第二个矩阵的每一行之间的内积（即余弦相似度）
# 因此输出形状为(152, 152)，表示每个酒店与所有其他酒店（包括自己）的相似度
print(cosine_similarities)
print(cosine_similarities.shape)

indices = pd.Series(df.index) #df.index是酒店名称
```

这段代码这段代码使用余弦相似度计算酒店之间的相似性。

为什么输入和输出的维度不同？

输入维度 (152, 3173)：
第一维 152：表示有 152 个酒店
第二维 3173：表示每个酒店的 TF-IDF 特征维度

输出维度 (152, 152)：
第一维 152：表示每个酒店
第二维 152：表示与所有其他酒店的相似度得分

具体解释：
- linear_kernel(A, B) 计算矩阵 A 和 B 之间每行的余弦相似度
- 当 A = B 时，计算每个酒店与所有酒店的相似度
- 结果矩阵中第 i 行第 j 列的值表示第 i 个酒店与第 j 个酒店的相似度
- 对角线元素（i=j）为 1.0，表示酒店与自己的相似度是完全相同的
- 这种设计使得我们可以快速找到与特定酒店最相似的其他酒店。

计算公式： 余弦相似度 = (A·B) / (||A|| × ||B||)

其中：

A·B 是两个向量的点积
||A|| 和 ||B|| 分别是两个向量的模长
关键点：

每行代表一个酒店
每列代表与该酒店的相似度得分
对角线元素都是1（自己与自己的相似度）
矩阵是对称的（酒店A与酒店B的相似度 = 酒店B与酒店A的相似度）

```text
# 假设我们有3个酒店的例子来说明计算过程

# 1. 输入矩阵 (3, 5) - 3个酒店，每个有5维特征
# tfidf_matrix = [
#     [0.1, 0.2, 0.3, 0.0, 0.4],  # 酒店1的特征向量
#     [0.2, 0.1, 0.0, 0.5, 0.3],  # 酒店2的特征向量  
#     [0.0, 0.3, 0.2, 0.4, 0.1]   # 酒店3的特征向量
# ]

# 2. 计算过程 - 以计算酒店1与所有酒店的相似度为例

# 酒店1 vs 酒店1 (对角线元素)
# cos_similarity = (A·B) / (||A|| × ||B||)
# A = [0.1, 0.2, 0.3, 0.0, 0.4]
# B = [0.1, 0.2, 0.3, 0.0, 0.4]
# A·B = 0.1×0.1 + 0.2×0.2 + 0.3×0.3 + 0.0×0.0 + 0.4×0.4 = 0.01+0.04+0.09+0+0.16 = 0.3 (两两相乘然后相加)
# ||A|| = sqrt(0.1² + 0.2² + 0.3² + 0.0² + 0.4²) = sqrt(0.01+0.04+0.09+0+0.16) = sqrt(0.3) ≈ 0.5477 （每一个元素平方后相加然后开方）
# ||B|| = sqrt(0.1² + 0.2² + 0.3² + 0.0² + 0.4²) = sqrt(0.3) ≈ 0.5477
# cos_similarity = 0.3 / (0.5477 × 0.5477) = 0.3 / 0.3 = 1.0

# 酒店1 vs 酒店2
# A = [0.1, 0.2, 0.3, 0.0, 0.4]  
# B = [0.2, 0.1, 0.0, 0.5, 0.3]
# A·B = 0.1×0.2 + 0.2×0.1 + 0.3×0.0 + 0.0×0.5 + 0.4×0.3 = 0.02+0.02+0+0+0.12 = 0.16
# ||A|| = sqrt(0.3) ≈ 0.5477
# ||B|| = sqrt(0.2² + 0.1² + 0.0² + 0.5² + 0.3²) = sqrt(0.04+0.01+0+0.25+0.09) = sqrt(0.39) ≈ 0.6245
# cos_similarity = 0.16 / (0.5477 × 0.6245) ≈ 0.16 / 0.3417 ≈ 0.468

# 最终相似度矩阵 (3, 3):
# [
#     [1.0, 0.468, 0.234],  # 酒店1与所有酒店的相似度
#     [0.468, 1.0, 0.321],  # 酒店2与所有酒店的相似度  
#     [0.234, 0.321, 1.0]   # 酒店3与所有酒店的相似度
# ]

# 对于原始数据 (152, 3173) -> (152, 152)
# 每个酒店(152维)与所有152个酒店计算相似度
# 结果是一个152×152的对称矩阵，其中每个元素表示两个酒店间的相似度
```
