```py
def get_top_n_words(corpus, n=1, k=None):
    """
    Extracts and returns the top k most frequent n-grams (e.g., words, bigrams, trigrams)
    from a given text corpus.

    Parameters
    ----------
    corpus : list[str]
        A list of text documents (each element is a string). Example:
        ["I love cats", "I love dogs too"]

    n : int, optional (default=1)
        The 'n' in n-gram.
        - n=1 → unigrams (single words)
        - n=2 → bigrams (two-word phrases)
        - n=3 → trigrams (three-word phrases), etc.

    k : int or None, optional (default=None)
        The number of top frequent words or n-grams to return.
        If None, returns all words sorted by frequency.

    Returns
    -------
    list[tuple[str, int]]
        A list of tuples: [(word_or_ngram, frequency), ...]
        Sorted by frequency in descending order.

    Example
    -------
    >>> corpus = ["I love cats", "I love dogs too"]
    >>> get_top_n_words(corpus, n=1, k=3)
    [('love', 2), ('i', 2), ('cats', 1)]
    """

    # Initialize a CountVectorizer to count occurrences of n-grams in the corpus.
    # - ngram_range=(n, n): ensures we only consider n-grams of length exactly 'n'.
    # - stop_words: removes common English stopwords (like 'the', 'and', 'is').
    vec = CountVectorizer(ngram_range=(n, n), stop_words=list(ENGLISH_STOPWORDS)).fit(corpus)

    # Transform the corpus into a bag-of-words matrix (documents × n-grams)
    # Each entry represents how many times an n-gram appears in that document.
    bag_of_words = vec.transform(corpus)

    # Sum all columns (axis=0) to get total count of each n-gram across all documents.
    sum_words = bag_of_words.sum(axis=0)

    # Build a list of (word_or_ngram, frequency) pairs.
    # vec.vocabulary_ maps each n-gram to its column index in the bag_of_words matrix.
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]

    # Sort by frequency in descending order (most frequent first).
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

    # Return only the top 'k' results (or all if k=None).
    return words_freq[:k]
```

## What is CountVectorizer ##

💡 简单解释

CountVectorizer 是 Scikit-learn 里一个把**文本（text）转换成数字特征(numeric features)**的工具。
它的主要作用是把句子或文章变成“词频向量”（bag-of-words representation）。

通俗来说，它做了三件事：

### 1. 分词 (Tokenization)：###
把每篇文本拆分成单词或短语（根据你设定的 ngram_range）。
例如：

```
["I love cats", "I love dogs too"]
```

🟩 unigram（单词级别）

ngram_range=(1, 1)
每次取一个词。

```
["i", "love", "cats", "dogs", "too"]
```

🟦 bigram（双词组合）

ngram_range=(2, 2)
每次取两个连续的词。

```
["i love", "love cats", "love dogs", "dogs too"]
```

🟧 trigram（三词组合）

ngram_range=(3, 3)
每次取三个连续的词。

```
["i love cats", "i love dogs", "love dogs too"]
```

#### 2. 建立词表 (Build vocabulary) ####

统计所有出现过的词或短语，并给每个分配一个唯一编号。

{'i': 0, 'love': 1, 'cats': 2, 'dogs': 3, 'too': 4}


#### 3. 统计词频 (Count occurrences)：####
数出每个词在每篇文本中出现了几次，生成一个“稀疏矩阵”（sparse matrix）。
比如：

- 文本1: I love cats
- 文本2: I love dogs dogs too


转成矩阵：

|     | i | love | cats | dogs | too |
| --- | - | ---- | ---- | ---- | --- |
| 文本1 | 1 | 1    | 1    | 0    | 0   |
| 文本2 | 1 | 1    | 0    | 2    | 1   |


🧩 参数重点
| 参数                            | 说明                                               |
| ----------------------------- | ------------------------------------------------ |
| `ngram_range=(1, 1)`          | 表示只统计单个词（unigram）；如果设 `(2, 2)` 就是统计双词组合（bigram）。 |
| `stop_words='english'` 或自定义列表 | 指定常见的停用词（如 *the, is, and*）在统计时被忽略。               |
| `max_features`                | 限制最多保留多少个高频词。                                    |
| `min_df` / `max_df`           | 控制某个词在多少文档中出现才被统计，过滤掉太少或太常见的词。                   |

🧠 输出结果
`CountVectorizer.fit_transform(corpus)`

返回一个稀疏矩阵（每行代表一篇文档（在这个案例是CSV文件中的一行），每列代表一个词），数值是该词在该文档出现的次数。

🧾 小例子

```py
from sklearn.feature_extraction.text import CountVectorizer

corpus = ["I love cats", "I love dogs too"]
vec = CountVectorizer()
X = vec.fit_transform(corpus)

print(vec.get_feature_names_out())  # 查看词表
# ['cats' 'dogs' 'i' 'love' 'too']

print(X.toarray())  # 查看词频矩阵
# [[1 0 1 1 0]
#  [0 1 1 1 1]]
```

#### CountVectorizer或TfidfVectorizer的实例主要包括以下重要内容：####

# 1. vocabulary_ - 词汇表字典
#    格式：{词汇: 索引位置}
# vec.vocabulary_.items() 返回的是：{词汇: 词频索引} 的字典映射， 例如 dict_items([('word1', 0), ('word2', 1), ...])
vocabulary = vec.vocabulary_  

# 2. get_feature_names_out() - 特征名称列表
#    返回按索引排序的词汇列表， 例如 ['word1', 'word2', ...]
feature_names = vec.get_feature_names_out()

# 3. transform() 方法 - 文本向量化
#    将文本转换为稀疏矩阵表示
transformed_matrix = vec.transform(texts)

# 4. fit_transform() 方法 - 训练并转换
#    先拟合词汇表，再进行向量化
fitted_transformed = vec.fit_transform(texts)

# 5. fit() 方法 - 拟合词汇表
#    根据训练数据建立词汇表
vec.fit(texts)

# 6. 向量化结果的形状信息
#    矩阵维度：(文档数 × 词汇表大小)
matrix_shape = vec.transform(texts).shape

# 7. 其他重要属性：
#    - vocabulary_：词汇到索引的映射
#    - stop_words_：停用词集合（如果使用了停用词）
#    - n_features_：特征数量


### 统计词频 (Count occurrences)：###

```py
    # Sum all columns (axis=0) to get total count of each n-gram across all documents.
    sum_words = bag_of_words.sum(axis=0)
```
bag_of_words: 这是一个词袋模型的矩阵，通常形状为 (文档数, 词汇数)。在这个案例当中，文档数为csv的行数，152行，即CSV当中152个酒店的desc. 而3-gram的词汇数为13908. 每一列代表一个特定的n-gram（词组），也是这个模型的一个特征值。

   axis=0: 这个参数指定沿着行的方向进行求和，对每一列（每个n-gram）的所有行进行加总， 最后得到一个长度为13908的数组，表示每个n-gram在整个词汇表中出现的总次数。 也就是每个特征值的总出现次数。 也就是每个特征值的总频。

假设 bag_of_words 矩阵是这样的（3个文档，4个n-gram）：

```py
        n-gram_A  n-gram_B  n-gram_C  n-gram_D
文档1:     [2]        [0]        [1]        [3]
文档2:     [1]        [2]        [0]        [1]  
文档3:     [0]        [1]        [2]        [2]

```

执行 bag_of_words.sum(axis=0) 后：

n-gram_A: 2 + 1 + 0 = 3
n-gram_B: 0 + 2 + 1 = 3
n-gram_C: 1 + 0 + 2 = 3
n-gram_D: 3 + 1 + 2 = 6

结果：sum_words = [3, 3, 3, 6]

注意：此时sum_words还是一个稀疏矩阵，但它的维度发生了变化：
# - bag_of_words: (文档数 × 词汇表大小) 的稀疏矩阵
# - sum_words: (1 × 词汇表大小) 的稀疏矩阵

#### 实际意义 ####
这一步的目的是：

- 统计全局频率：计算每个n-gram在整个语料库中的总出现次数
- 特征工程：为后续的文本分析、机器学习模型提供基础统计信息
- 降维处理：从文档级别的词频矩阵转换为词汇级别的频率统计

#### 矩阵解释 ####

每一行代表一个文档，每一列代表一个特定的n-gram：

文档1：n-gram_A出现2次，n-gram_B出现0次，n-gram_C出现1次，n-gram_D出现3次
文档2：n-gram_A出现1次，n-gram_B出现2次，n-gram_C出现0次，n-gram_D出现1次
文档3：n-gram_A出现0次，n-gram_B出现1次，n-gram_C出现2次，n-gram_D出现2次

这样就得到了每个文档中各个n-gram的精确词频统计。
- 每个文档的每个n-gram都有具体的出现次数
- 这些计数反映了实际文本中词汇的频率分布
- 通过 sum(axis=0) 操作，我们得到了每个n-gram在整个语料库中的总出现次数



```py
 # Build a list of (word_or_ngram, frequency) pairs.
    # vec.vocabulary_ is a dictionary that maps each unique n-gram (word or phrase) 
    # to its corresponding column index in the bag_of_words matrix. 
    # This mapping allows us to efficiently access the frequency count of each n-gram 
    # by using the column index retrieved from this dictionary.
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
```
这行代码的意思是：

遍历 vec.vocabulary_ 中的每一个 (feature_name, feature_idx) 对。
通过索引 feature_idx，从 sum_words[0, idx] 获取该feature（n-gram）的总出现次数。
sum_words[0, idx] 表示取稀疏矩阵 sum_words 的第 0 行、第 idx 列的值。
然后将 (feature_name, count) 组成元组，存入 words_freq 列表。

相当于

```py
# Build a list of (word_or_ngram, frequency) pairs using a simple loop
words_freq = []
for word, idx in vec.vocabulary_.items():
    # Extract frequency for this specific feature using the index
    freq = sum_words[0, idx]
    words_freq.append((word, freq))
```